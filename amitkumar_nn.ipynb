Problem statement: To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.

Importing Skin Cancer Data
To do: Take necessary actions to read the data
Importing all the important libraries
#Importing all the important libraries
import pathlib
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os, cv2
import PIL
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.models import Sequential

import warnings
warnings.filterwarnings("ignore")
## If you are using the data by mounting the google drive, use the following :
from google.colab import drive
drive.mount('/content/gdrive')

##Ref:https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166
Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount("/content/gdrive", force_remount=True).
This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively.

# Defining the path for train and test images
## Todo: Update the paths of the train and test dataset
data_dir_train = pathlib.Path("/content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train")
data_dir_test = pathlib.Path('/content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Test')
image_count_train = len(list(data_dir_train.glob('*/*.jpg')))
print(image_count_train)
image_count_test = len(list(data_dir_test.glob('*/*.jpg')))
print(image_count_test)
2239
118
Load using keras.preprocessing
Let's load these images off disk using the helpful image_dataset_from_directory utility.

Create a dataset
Define some parameters for the loader:

# Set image size and batch size
image_size = (180, 180)
batch_size = 32
Use 80% of the images for training, and 20% for validation.

## Write your train dataset here
## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory
## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir_train,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=image_size,
    batch_size=batch_size)
Found 2239 files belonging to 9 classes.
Using 1792 files for training.
## Write your validation dataset here
## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory
## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir_train,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=image_size,
    batch_size=batch_size)
Found 2239 files belonging to 9 classes.
Using 447 files for validation.
# Loading the testing data
# using seed=123 while creating dataset using tf.keras.preprocessing.image_dataset_from_directory
# resizing images to the size img_height*img_width, while writting the dataset
test_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir_test,
                                                             seed=123,
                                                             image_size=image_size,
                                                             batch_size=batch_size)
Found 118 files belonging to 9 classes.
# List out all the classes of skin cancer and store them in a list.
# You can find the class names in the class_names attribute on these datasets.
# These correspond to the directory names in alphabetical order.
class_names = train_ds.class_names
print(class_names)
['actinic keratosis', 'basal cell carcinoma', 'dermatofibroma', 'melanoma', 'nevus', 'pigmented benign keratosis', 'seborrheic keratosis', 'squamous cell carcinoma', 'vascular lesion']
Visualize the data
Todo, create a code to visualize one instance of all the nine classes present in the dataset
image = plt.imread((list(data_dir_train.glob(class_names[0]+'/*.jpg'))[0]))
plt.imshow(image)
<matplotlib.image.AxesImage at 0x7af1dacd2140>

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")

The image_batch is a tensor of the shape (32, 180, 180, 3). This is a batch of 32 images of shape 180x180x3 (the last dimension refers to color channels RGB). The label_batch is a tensor of the shape (32,), these are corresponding labels to the 32 images.

Dataset.cache() keeps the images in memory after they're loaded off disk during the first epoch.

Dataset.prefetch() overlaps data preprocessing and model execution while training.

AUTOTUNE = tf.data.experimental.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
Create the model
Todo: Create a CNN model, which can accurately detect 9 classes present in the dataset. Use layers.experimental.preprocessing.Rescaling to normalize pixel values between (0,1). The RGB channel values are in the [0, 255] range. This is not ideal for a neural network. Here, it is good to standardize values to be in the [0, 1]
# CNN Model
model=models.Sequential()
# scaling the pixel values from 0-255 to 0-1
model.add(layers.Rescaling(scale=1./255,input_shape=(180,180,3)))

# Convolution layer with 64 features, 3x3 filter and relu activation with 2x2 pooling
model.add(layers.Conv2D(64,(3,3),padding = 'same',activation='relu'))
model.add(layers.MaxPooling2D())

# Convolution layer with 128 features, 3x3 filter and relu activation with 2x2 pooling
model.add(layers.Conv2D(128,(3,3),padding = 'same',activation='relu'))
model.add(layers.MaxPooling2D())

model.add(layers.Flatten())
model.add(layers.Dense(256,activation='relu'))
model.add(layers.Dense(9,activation='softmax'))
Compile the model
Choose an appropirate optimiser and loss function for model training

### Todo, choose an appropirate optimiser and loss function
# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
# View the summary of all layers
model.summary()
Model: "sequential_10"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ rescaling_7 (Rescaling)              │ (None, 180, 180, 3)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_16 (Conv2D)                   │ (None, 180, 180, 64)        │           1,792 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_16 (MaxPooling2D)      │ (None, 90, 90, 64)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_17 (Conv2D)                   │ (None, 90, 90, 128)         │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_17 (MaxPooling2D)      │ (None, 45, 45, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_7 (Flatten)                  │ (None, 259200)              │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_14 (Dense)                     │ (None, 256)                 │      66,355,456 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_15 (Dense)                     │ (None, 9)                   │           2,313 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 66,433,417 (253.42 MB)
 Trainable params: 66,433,417 (253.42 MB)
 Non-trainable params: 0 (0.00 B)
Train the model
epochs = 20
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)
Epoch 1/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 359s 2s/step - accuracy: 0.2308 - loss: 4.7015 - val_accuracy: 0.4318 - val_loss: 1.6582
Epoch 2/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 27s 68ms/step - accuracy: 0.4421 - loss: 1.6098 - val_accuracy: 0.4855 - val_loss: 1.5170
Epoch 3/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 4s 64ms/step - accuracy: 0.5147 - loss: 1.4021 - val_accuracy: 0.5414 - val_loss: 1.3548
Epoch 4/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 5s 65ms/step - accuracy: 0.5918 - loss: 1.1815 - val_accuracy: 0.5257 - val_loss: 1.4500
Epoch 5/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 4s 66ms/step - accuracy: 0.5881 - loss: 1.1674 - val_accuracy: 0.4407 - val_loss: 1.6522
Epoch 6/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 5s 65ms/step - accuracy: 0.6401 - loss: 1.0650 - val_accuracy: 0.5280 - val_loss: 1.4864
Epoch 7/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 4s 67ms/step - accuracy: 0.6932 - loss: 0.8592 - val_accuracy: 0.5101 - val_loss: 1.6449
Epoch 8/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 4s 66ms/step - accuracy: 0.7088 - loss: 0.7909 - val_accuracy: 0.5101 - val_loss: 1.5661
Epoch 9/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 5s 66ms/step - accuracy: 0.7774 - loss: 0.6399 - val_accuracy: 0.5280 - val_loss: 1.8473
Epoch 10/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 5s 68ms/step - accuracy: 0.8060 - loss: 0.5518 - val_accuracy: 0.4989 - val_loss: 2.0100
Epoch 11/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 5s 70ms/step - accuracy: 0.8203 - loss: 0.5362 - val_accuracy: 0.5324 - val_loss: 1.7268
Epoch 12/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 5s 66ms/step - accuracy: 0.8302 - loss: 0.4725 - val_accuracy: 0.5324 - val_loss: 1.7909
Epoch 13/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 5s 68ms/step - accuracy: 0.8700 - loss: 0.3586 - val_accuracy: 0.5526 - val_loss: 2.0675
Epoch 14/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 4s 68ms/step - accuracy: 0.8774 - loss: 0.3334 - val_accuracy: 0.5548 - val_loss: 1.8645
Epoch 15/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 5s 66ms/step - accuracy: 0.8646 - loss: 0.3079 - val_accuracy: 0.5190 - val_loss: 2.0058
Epoch 16/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 4s 68ms/step - accuracy: 0.9110 - loss: 0.2158 - val_accuracy: 0.5660 - val_loss: 2.1261
Epoch 17/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 4s 68ms/step - accuracy: 0.9034 - loss: 0.2358 - val_accuracy: 0.5369 - val_loss: 2.4345
Epoch 18/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 5s 66ms/step - accuracy: 0.9269 - loss: 0.1898 - val_accuracy: 0.5190 - val_loss: 2.4234
Epoch 19/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 4s 68ms/step - accuracy: 0.8995 - loss: 0.2471 - val_accuracy: 0.5056 - val_loss: 2.5449
Epoch 20/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 5s 71ms/step - accuracy: 0.9133 - loss: 0.2151 - val_accuracy: 0.4877 - val_loss: 2.5158
Visualizing training results
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

Todo: Write your findings after the model fit, see if there is an evidence of model overfit or underfit
Write your findings here
-> Training accuracy is increasing linearly, but validation accuracy remains around 45 to 55% mark

-> As epochs increases, training loss increases and validation loss increases

-> This looks like overfitting model as there is a substancial difference between training and validation accuracy

# Todo, after you have analysed the model fit history for presence of underfit or overfit, choose an appropriate data augumentation strategy.
image_height = 180
image_width = 180
data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",input_shape=(image_height,image_width,3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)
# Todo, visualize how your augmentation strategy works for one instance of training image.
plt.figure(figsize=(10, 10))
for images, _ in train_ds.take(1):
  for i in range(9):
    augmented_images = data_augmentation(images)
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(augmented_images[0].numpy().astype("uint8"))
    plt.axis("off")

Todo:
Create the model, compile and train the model
## You can use Dropout layer if there is an evidence of overfitting in your findings

# CNN Model
model=models.Sequential()
# scaling the pixel values from 0-255 to 0-1
model.add(layers.Rescaling(scale=1./255,input_shape=(180,180,3)))

# adding the augmentation layer before the convolution layer
model.add(data_augmentation)

# Convolution layer with 64 features, 3x3 filter and relu activation with 2x2 pooling
model.add(layers.Conv2D(64,(3,3),padding = 'same',activation='relu'))
model.add(layers.MaxPooling2D())

# Convolution layer with 128 features, 3x3 filter and relu activation with 2x2 pooling
model.add(layers.Conv2D(128,(3,3),padding = 'same',activation='relu'))
model.add(layers.MaxPooling2D())

model.add(layers.Flatten())
model.add(layers.Dense(256,activation='relu'))
model.add(layers.Dense(9,activation='softmax'))
Compiling the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])
model.summary()
Model: "sequential_12"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ rescaling_8 (Rescaling)              │ (None, 180, 180, 3)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ sequential_11 (Sequential)           │ (None, 180, 180, 3)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_18 (Conv2D)                   │ (None, 180, 180, 64)        │           1,792 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_18 (MaxPooling2D)      │ (None, 90, 90, 64)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_19 (Conv2D)                   │ (None, 90, 90, 128)         │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_19 (MaxPooling2D)      │ (None, 45, 45, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_8 (Flatten)                  │ (None, 259200)              │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_16 (Dense)                     │ (None, 256)                 │      66,355,456 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_17 (Dense)                     │ (None, 9)                   │           2,313 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 66,433,417 (253.42 MB)
 Trainable params: 66,433,417 (253.42 MB)
 Non-trainable params: 0 (0.00 B)
Training the model
## Your code goes here, note: train your model for 20 epochs
epochs = 20
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)
Epoch 1/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 7s 104ms/step - accuracy: 0.1951 - loss: 6.4651 - val_accuracy: 0.2371 - val_loss: 1.9447
Epoch 2/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 10s 100ms/step - accuracy: 0.3737 - loss: 1.7633 - val_accuracy: 0.3221 - val_loss: 1.7978
Epoch 3/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 102ms/step - accuracy: 0.4132 - loss: 1.6713 - val_accuracy: 0.4944 - val_loss: 1.5147
Epoch 4/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 101ms/step - accuracy: 0.4771 - loss: 1.4928 - val_accuracy: 0.4720 - val_loss: 1.5309
Epoch 5/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 10s 103ms/step - accuracy: 0.4386 - loss: 1.5431 - val_accuracy: 0.5235 - val_loss: 1.4216
Epoch 6/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 100ms/step - accuracy: 0.4989 - loss: 1.3696 - val_accuracy: 0.5145 - val_loss: 1.3982
Epoch 7/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 102ms/step - accuracy: 0.5256 - loss: 1.3524 - val_accuracy: 0.5414 - val_loss: 1.3497
Epoch 8/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 100ms/step - accuracy: 0.5535 - loss: 1.2704 - val_accuracy: 0.5257 - val_loss: 1.4095
Epoch 9/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 101ms/step - accuracy: 0.5360 - loss: 1.2405 - val_accuracy: 0.5369 - val_loss: 1.3466
Epoch 10/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 10s 101ms/step - accuracy: 0.5470 - loss: 1.2262 - val_accuracy: 0.5324 - val_loss: 1.4431
Epoch 11/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 101ms/step - accuracy: 0.5694 - loss: 1.2177 - val_accuracy: 0.5280 - val_loss: 1.3814
Epoch 12/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 10s 101ms/step - accuracy: 0.5689 - loss: 1.1979 - val_accuracy: 0.5660 - val_loss: 1.2812
Epoch 13/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 101ms/step - accuracy: 0.5756 - loss: 1.1943 - val_accuracy: 0.5078 - val_loss: 1.4592
Epoch 14/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 100ms/step - accuracy: 0.5727 - loss: 1.1713 - val_accuracy: 0.5459 - val_loss: 1.3224
Epoch 15/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 102ms/step - accuracy: 0.6012 - loss: 1.1286 - val_accuracy: 0.4810 - val_loss: 1.4380
Epoch 16/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 101ms/step - accuracy: 0.5883 - loss: 1.1607 - val_accuracy: 0.5459 - val_loss: 1.3495
Epoch 17/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 101ms/step - accuracy: 0.6042 - loss: 1.0972 - val_accuracy: 0.5548 - val_loss: 1.4382
Epoch 18/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 101ms/step - accuracy: 0.6190 - loss: 1.0596 - val_accuracy: 0.5213 - val_loss: 1.4360
Epoch 19/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 10s 102ms/step - accuracy: 0.6401 - loss: 1.0259 - val_accuracy: 0.5369 - val_loss: 1.3936
Epoch 20/20
56/56 ━━━━━━━━━━━━━━━━━━━━ 6s 100ms/step - accuracy: 0.6304 - loss: 1.0192 - val_accuracy: 0.5302 - val_loss: 1.4120
Visualizing the results
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

Todo: Write your findings after the model fit, see if there is an evidence of model overfit or underfit. Do you think there is some improvement now as compared to the previous model run?
Findings:
-> From the model, there is no substancial difference between training accuracy and validation accuracy and difference which is slightly noticeable

-> As the no of epochs increases, we afre observing the gap between training loss and validation loss, which could be sign of overfitting

Todo: Find the distribution of classes in the training dataset.
Context: Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data.
# CNN Model
model=models.Sequential()
# scaling the pixel values from 0-255 to 0-1
model.add(layers.Rescaling(scale=1./255,input_shape=(180,180,3)))
model.add(data_augmentation)

# Convolution layer with 64 features, 3x3 filter and relu activation with 2x2 pooling
model.add(layers.Conv2D(64,(3,3),padding = 'same',activation='relu'))
model.add(layers.MaxPooling2D())

# Convolution layer with 128 features, 3x3 filter and relu activation with 2x2 pooling
model.add(layers.Conv2D(128,(3,3),padding = 'same',activation='relu'))
model.add(layers.MaxPooling2D())
#adding a 20% dropout after the convolution layers
model.add(layers.Dropout(0.2))

model.add(layers.Flatten())
model.add(layers.Dense(256,activation='relu'))
model.add(layers.Dense(9,activation='softmax'))
# Compiling the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])
model.summary()
Model: "sequential_13"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ rescaling_9 (Rescaling)              │ (None, 180, 180, 3)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ sequential_11 (Sequential)           │ (None, 180, 180, 3)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_20 (Conv2D)                   │ (None, 180, 180, 64)        │           1,792 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_20 (MaxPooling2D)      │ (None, 90, 90, 64)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_21 (Conv2D)                   │ (None, 90, 90, 128)         │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_21 (MaxPooling2D)      │ (None, 45, 45, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_2 (Dropout)                  │ (None, 45, 45, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_9 (Flatten)                  │ (None, 259200)              │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_18 (Dense)                     │ (None, 256)                 │      66,355,456 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_19 (Dense)                     │ (None, 9)                   │           2,313 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 66,433,417 (253.42 MB)
 Trainable params: 66,433,417 (253.42 MB)
 Non-trainable params: 0 (0.00 B)
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

Todo: Write your findings here:
- Which class has the least number of samples?
-> seborrheic keratosis has the least number of samples

- Which classes dominate the data in terms proportionate number of samples?
-> pigmented benign keratosis dominate the data in number of samples

for i in range(len(class_names)):
  print(class_names[i],' ==> ',len(list(data_dir_train.glob(class_names[i]+'/*.jpg'))))
actinic keratosis  ==>  114
basal cell carcinoma  ==>  376
dermatofibroma  ==>  95
melanoma  ==>  438
nevus  ==>  357
pigmented benign keratosis  ==>  462
seborrheic keratosis  ==>  77
squamous cell carcinoma  ==>  181
vascular lesion  ==>  139
Todo: Rectify the class imbalance
Context: You can use a python package known as Augmentor (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples.
!pip install Augmentor
Requirement already satisfied: Augmentor in /usr/local/lib/python3.10/dist-packages (0.2.12)
Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from Augmentor) (9.4.0)
Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from Augmentor) (4.66.4)
Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from Augmentor) (1.26.4)
To use Augmentor, the following general procedure is followed:

Instantiate a Pipeline object pointing to a directory containing your initial image data set.
Define a number of operations to perform on this data set using your Pipeline object.
Execute these operations by calling the Pipeline’s sample() method.
import Augmentor
for i in class_names:
    p = Augmentor.Pipeline(data_dir_train)
    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)
    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse.
Initialised with 2239 image(s) found.
Output directory set to /content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train/output.
Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7AF2567ED150>: 100%|██████████| 500/500 [00:50<00:00,  9.81 Samples/s]
Initialised with 2239 image(s) found.
Output directory set to /content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train/output.
Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7AF1E4308520>: 100%|██████████| 500/500 [00:47<00:00, 10.55 Samples/s]
Initialised with 2239 image(s) found.
Output directory set to /content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train/output.
Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7AF254726E00>: 100%|██████████| 500/500 [00:46<00:00, 10.78 Samples/s]
Initialised with 2239 image(s) found.
Output directory set to /content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train/output.
Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x450 at 0x7AF2567C8A30>: 100%|██████████| 500/500 [00:50<00:00,  9.88 Samples/s]
Initialised with 2239 image(s) found.
Output directory set to /content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train/output.
Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7AF2D865FFD0>: 100%|██████████| 500/500 [00:39<00:00, 12.60 Samples/s]
Initialised with 2239 image(s) found.
Output directory set to /content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train/output.
Processing <PIL.Image.Image image mode=RGB size=3872x2592 at 0x7AF2F6CE4A90>: 100%|██████████| 500/500 [00:46<00:00, 10.79 Samples/s]
Initialised with 2239 image(s) found.
Output directory set to /content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train/output.
Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7AF1D8FAB730>: 100%|██████████| 500/500 [00:41<00:00, 12.01 Samples/s]
Initialised with 2239 image(s) found.
Output directory set to /content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train/output.
Processing <PIL.Image.Image image mode=RGB size=1024x768 at 0x7AF1D89DBBE0>: 100%|██████████| 500/500 [00:43<00:00, 11.38 Samples/s]
Initialised with 2239 image(s) found.
Output directory set to /content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train/output.
Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x450 at 0x7AF1DBA9E500>: 100%|██████████| 500/500 [00:49<00:00, 10.07 Samples/s]
Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images.

data_dir_train1 = pathlib.Path("/content/gdrive/MyDrive/Skin cancer ISIC The International Skin Imaging Collaboration/Train/output")
image_count_train = len(list(data_dir_train1.glob('*/*.jpg')))
print(image_count_train)
4500
Lets see the distribution of augmented data after adding new images to the original training data.
from glob import glob
path_list = [x for x in glob(os.path.join(data_dir_train1, '*', '*.jpg'))]
lesion_list_new = [os.path.basename(os.path.dirname(y)) for y in glob(os.path.join(data_dir_train1, '*', '*.jpg'))]
dataframe_dict_new = dict(zip(path_list, lesion_list_new))
new_df = pd.DataFrame(list(dataframe_dict_new.items()),columns = ['Path','Label'])
new_df['Label'].value_counts()
Label
pigmented benign keratosis    887
melanoma                      840
basal cell carcinoma          795
nevus                         730
squamous cell carcinoma       350
vascular lesion               314
actinic keratosis             238
dermatofibroma                203
seborrheic keratosis          143
Name: count, dtype: int64
So, now we have added 500 images to all the classes to maintain some class balance. We can add more images as we want to improve training process.

Todo: Train the model on the data created using Augmentor
batch_size = 32
img_height = 180
img_width = 180
Todo: Create a training dataset
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir_train1,
  seed=123,
  validation_split = 0.2,
  subset = "training",
  image_size=(img_height, img_width),
  batch_size=batch_size)
Found 4500 files belonging to 9 classes.
Using 3600 files for training.
Todo: Create a validation dataset
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir_train1,
  seed=123,
  validation_split = 0.2,
  subset = "validation",
  image_size=(img_height, img_width),
  batch_size=batch_size)
Found 4500 files belonging to 9 classes.
Using 900 files for validation.
Todo: Create your model (make sure to include normalization)
# CNN Model
model=models.Sequential()
# scaling the pixel values from 0-255 to 0-1
model.add(layers.Rescaling(scale=1./255,input_shape=(180,180,3)))
model.add(data_augmentation)

# Convolution layer with 64 features, 3x3 filter and relu activation with 2x2 pooling
model.add(layers.Conv2D(64,(3,3),padding = 'same',activation='relu'))
model.add(layers.MaxPooling2D())

# Convolution layer with 128 features, 3x3 filter and relu activation with 2x2 pooling
model.add(layers.Conv2D(128,(3,3),padding = 'same',activation='relu'))
model.add(layers.MaxPooling2D())
#adding a 20% dropout after the convolution layers
model.add(layers.Dropout(0.2))

model.add(layers.Flatten())
model.add(layers.Dense(256,activation='relu'))
model.add(layers.Dense(9,activation='softmax'))
Todo: Compile your model (Choose optimizer and loss function appropriately)
# Compiling the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])
model.summary()
Model: "sequential_14"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ rescaling_10 (Rescaling)             │ (None, 180, 180, 3)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ sequential_11 (Sequential)           │ (None, 180, 180, 3)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_22 (Conv2D)                   │ (None, 180, 180, 64)        │           1,792 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_22 (MaxPooling2D)      │ (None, 90, 90, 64)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_23 (Conv2D)                   │ (None, 90, 90, 128)         │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_23 (MaxPooling2D)      │ (None, 45, 45, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_3 (Dropout)                  │ (None, 45, 45, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_10 (Flatten)                 │ (None, 259200)              │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_20 (Dense)                     │ (None, 256)                 │      66,355,456 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_21 (Dense)                     │ (None, 9)                   │           2,313 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 66,433,417 (253.42 MB)
 Trainable params: 66,433,417 (253.42 MB)
 Non-trainable params: 0 (0.00 B)
Todo: Train your model
epochs = 30
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)
Epoch 1/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 33s 279ms/step - accuracy: 0.2191 - loss: 4.1289 - val_accuracy: 0.3322 - val_loss: 1.8506
Epoch 2/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 28s 244ms/step - accuracy: 0.3960 - loss: 1.7176 - val_accuracy: 0.4722 - val_loss: 1.5880
Epoch 3/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 42s 258ms/step - accuracy: 0.4542 - loss: 1.5673 - val_accuracy: 0.5133 - val_loss: 1.4434
Epoch 4/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 38s 235ms/step - accuracy: 0.5003 - loss: 1.4133 - val_accuracy: 0.5211 - val_loss: 1.3397
Epoch 5/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 240ms/step - accuracy: 0.5066 - loss: 1.3576 - val_accuracy: 0.4667 - val_loss: 1.5236
Epoch 6/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 237ms/step - accuracy: 0.5475 - loss: 1.2837 - val_accuracy: 0.5311 - val_loss: 1.3368
Epoch 7/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 235ms/step - accuracy: 0.5564 - loss: 1.2349 - val_accuracy: 0.5744 - val_loss: 1.2031
Epoch 8/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 233ms/step - accuracy: 0.5729 - loss: 1.1992 - val_accuracy: 0.5622 - val_loss: 1.2505
Epoch 9/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 235ms/step - accuracy: 0.5856 - loss: 1.1824 - val_accuracy: 0.5589 - val_loss: 1.2347
Epoch 10/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 32s 280ms/step - accuracy: 0.5694 - loss: 1.1797 - val_accuracy: 0.5689 - val_loss: 1.1701
Epoch 11/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 28s 250ms/step - accuracy: 0.5996 - loss: 1.1151 - val_accuracy: 0.5644 - val_loss: 1.2277
Epoch 12/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 27s 242ms/step - accuracy: 0.6005 - loss: 1.1067 - val_accuracy: 0.5456 - val_loss: 1.2884
Epoch 13/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 42s 250ms/step - accuracy: 0.6121 - loss: 1.0763 - val_accuracy: 0.5789 - val_loss: 1.1991
Epoch 14/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 40s 238ms/step - accuracy: 0.6095 - loss: 1.0865 - val_accuracy: 0.5778 - val_loss: 1.2077
Epoch 15/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 42s 243ms/step - accuracy: 0.6251 - loss: 1.0313 - val_accuracy: 0.5978 - val_loss: 1.1487
Epoch 16/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 40s 235ms/step - accuracy: 0.6455 - loss: 0.9983 - val_accuracy: 0.6211 - val_loss: 1.1248
Epoch 17/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 27s 235ms/step - accuracy: 0.6564 - loss: 0.9687 - val_accuracy: 0.6289 - val_loss: 1.0814
Epoch 18/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 26s 232ms/step - accuracy: 0.6648 - loss: 0.9554 - val_accuracy: 0.5933 - val_loss: 1.1751
Epoch 19/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 233ms/step - accuracy: 0.6570 - loss: 0.9391 - val_accuracy: 0.6500 - val_loss: 1.0123
Epoch 20/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 236ms/step - accuracy: 0.6742 - loss: 0.8975 - val_accuracy: 0.6200 - val_loss: 1.0430
Epoch 21/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 236ms/step - accuracy: 0.6651 - loss: 0.9441 - val_accuracy: 0.6456 - val_loss: 1.0311
Epoch 22/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 235ms/step - accuracy: 0.6655 - loss: 0.8975 - val_accuracy: 0.6722 - val_loss: 0.9505
Epoch 23/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 46s 279ms/step - accuracy: 0.6766 - loss: 0.8610 - val_accuracy: 0.6533 - val_loss: 1.0778
Epoch 24/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 285ms/step - accuracy: 0.6894 - loss: 0.8604 - val_accuracy: 0.6256 - val_loss: 1.0741
Epoch 25/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 281ms/step - accuracy: 0.7017 - loss: 0.8403 - val_accuracy: 0.6722 - val_loss: 1.0146
Epoch 26/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 36s 235ms/step - accuracy: 0.7079 - loss: 0.8085 - val_accuracy: 0.6644 - val_loss: 1.0453
Epoch 27/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 237ms/step - accuracy: 0.7181 - loss: 0.7914 - val_accuracy: 0.6778 - val_loss: 0.9548
Epoch 28/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 41s 238ms/step - accuracy: 0.7307 - loss: 0.7832 - val_accuracy: 0.6856 - val_loss: 0.9157
Epoch 29/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 40s 234ms/step - accuracy: 0.7235 - loss: 0.7656 - val_accuracy: 0.7056 - val_loss: 0.9424
Epoch 30/30
113/113 ━━━━━━━━━━━━━━━━━━━━ 42s 245ms/step - accuracy: 0.7358 - loss: 0.7387 - val_accuracy: 0.6978 - val_loss: 0.9140
Todo: Visualize the model results
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

Todo: Analyze your results here. Did you get rid of underfitting/overfitting? Did class rebalance help?
 
-> Class rebalncing got rid of training loss and validation loss

-> Class rebalancing also helped to increase the accuracy to around 73%

-> substancial difference between training loss and validation loss decreased significantly
